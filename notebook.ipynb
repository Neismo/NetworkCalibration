{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Imports\n"
      ],
      "metadata": {
        "id": "CxcnVzgY_jKG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ICbwGmT8EDHB"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "\n",
        "import math\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from matplotlib.pyplot import imshow\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import CIFAR10\n",
        "from torchvision.transforms import ToTensor, Compose, RandomRotation, RandomHorizontalFlip, RandomCrop, Normalize\n",
        "from torchvision.transforms.functional import pil_to_tensor\n",
        "from torch.nn.modules.batchnorm import BatchNorm2d\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SZIXuuseBZPx",
        "outputId": "54a0967d-712d-4c07-d904-b5ad1f8eb7f8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load in CIFAR-10 - and prepare DataLoader\n",
        "### Transformation: RandomRotation (10 degrees)  \n",
        "### Transformation: RandomCrop (original size, but padded with 4 pixels on all sides  \n",
        "### Transformation: Horizontally Flipped\n",
        "#### All are normalized\n",
        "All loaded into a concatenated dataset and then shuffled using a DataLoader."
      ],
      "metadata": {
        "id": "UlhiYYZS_R85"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101,
          "referenced_widgets": [
            "bb120bb2b6c747e4897081f1daaddd5d",
            "51abb984d3134d0fba9f3507234986bf",
            "c00ebf1423a64e1d801a2d7c88de01d0",
            "fc504cb813b54098aa01f73bad7e517d",
            "bea9ff9ba1e74254b736f4ce49146530",
            "776b3662a007401d9f2bd5941181dae4",
            "e9da124cdf344622bdeab1a319139eed",
            "416ec78e87f14909b5d4305f20e16dbe",
            "311b7ab6fa2845678bc3dde13394e5c2",
            "4f27c12eb54e4393ac7d66d8119f22d0",
            "568ef96c155643fda76a98e883b6f427"
          ]
        },
        "id": "z_YwodLxEBeI",
        "outputId": "c95e2ddb-90a3-4dc9-918a-577c93556d3a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to /cifardata/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/170498071 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bb120bb2b6c747e4897081f1daaddd5d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /cifardata/cifar-10-python.tar.gz to /cifardata\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "# Download CIFAR-10 IF NOT ALREADY EXISTS\n",
        "raw_train = CIFAR10(\"/cifardata\", transform=Compose([ToTensor()]), download=True, train=True)\n",
        "train = CIFAR10(\"/cifardata\", transform=Compose([ToTensor(), Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2470, 0.2435, 0.2616])]), train=True) # We transform the data to tensors\n",
        "transformed_train = CIFAR10(\"/cifardata\", transform=Compose([ToTensor(), RandomCrop(32, 4, padding_mode=\"reflect\"), RandomHorizontalFlip(), Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2470, 0.2435, 0.2616])]), train=True)\n",
        "\n",
        "#train_hflip = CIFAR10(\"/cifardata\", transform=Compose([ToTensor(), RandomHorizontalFlip(1.0), Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2470, 0.2435, 0.2616])]), train=True) # We transform the data to tensors and horizontally flip them\n",
        "#train_rotate = CIFAR10(\"/cifardata\", transform=Compose([ToTensor(), RandomRotation(10), Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2470, 0.2435, 0.2616])]), train=True) # We transform the data to tensors and rotate them by 10 degrees\n",
        "#train_crop = CIFAR10(\"/cifardata\", transform=Compose([ToTensor(), RandomCrop(32, padding=4), Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2470, 0.2435, 0.2616])]), train=True) # We transform the data to tensors and crop with padding (remove some edges)\n",
        "#train_kaggle = CIFAR10(\"/cifardata\", transform=Compose([RandomCrop(32, padding=4, padding_mode=\"reflect\"), RandomHorizontalFlip(), ToTensor(), Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2470, 0.2435, 0.2616])]), train=True) # We transform the data to tensors\n",
        "\n",
        "raw_test = CIFAR10(\"/cifardata\", transform=Compose([ToTensor()]), download=True, train=False)\n",
        "test = CIFAR10(\"/cifardata\", transform=Compose([ToTensor(), Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2470, 0.2435, 0.2616])]), train=False) # Once downloaded.\n",
        "\n",
        "#concatDataset = torch.utils.data.ConcatDataset([train,train_hflip,train_rotate, train_crop])\n",
        "\n",
        "def get_train_loader(batch_size:int = 32):\n",
        "  return DataLoader(transformed_train, batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "\n",
        "def get_test_loader(batch_size=1):\n",
        "  return DataLoader(test, batch_size=batch_size, shuffle=False, drop_last=False)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Models"
      ],
      "metadata": {
        "id": "s8UGMfkn_ycz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## K Nearest Neighbour (Cosine Similarity) baseline model"
      ],
      "metadata": {
        "id": "fyUEDpixAP-J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "KNM = KNeighborsClassifier(n_neighbors=50,\n",
        "                             metric='cosine',\n",
        "                             algorithm='brute',\n",
        "                             n_jobs=-1)\n",
        "\n",
        "knn_data = np.array([raw_train[i][0].view(-1).numpy() for i in range(len(raw_train))])\n",
        "knn_labels = np.array([raw_train[i][1] for i in range(len(raw_train))])\n",
        "\n",
        "KNM.fit(knn_data, knn_labels)\n",
        "\n",
        "knn_test_data = np.array([raw_test[i][0].view(-1).numpy() for i in range(len(raw_test))])\n",
        "knn_test_labels = np.array([raw_test[i][1] for i in range(len(raw_test))])\n",
        "\n",
        "preds = KNM.predict(knn_test_data)\n",
        "\n",
        "correct, wrong = 0, 0\n",
        "for pred, true in zip(preds, knn_test_labels):\n",
        "  if pred == true:\n",
        "    correct += 1\n",
        "  else:\n",
        "    wrong += 1\n",
        "print(f\"W: {wrong}, C: {correct}, ACC: {(correct/(correct+wrong)) * 100.0}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Eq5YnejAVd6",
        "outputId": "7e2baf26-8287-484e-c1d9-396196367f17"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W: 6585, C: 3415, ACC: 34.150000000000006\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dense Neural Network (DNN)"
      ],
      "metadata": {
        "id": "ERRtmnDd0Unr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "mK3oxSx0R3dp"
      },
      "outputs": [],
      "source": [
        "class NNModel(nn.Module): ## Simple Feedforward Neural Network\n",
        "  def __init__(self):\n",
        "    super(NNModel, self).__init__()\n",
        "    self.flattener = nn.Flatten()\n",
        "    self.ln1 = nn.Linear(32*32*3, 512)\n",
        "    self.ln2 = nn.Linear(512, 512)\n",
        "    self.ln3 = nn.Linear(512, 10)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.flattener(x) # flatten to vector, input layer\n",
        "    x = F.relu(self.ln1(x)) # deep layer 1\n",
        "    x = F.relu(self.ln2(x)) # deep layer 2\n",
        "    x = self.ln3(x)# output layer\n",
        "    return x # return logits\n",
        "\n",
        "DNNModel = NNModel()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Simple Convolutional Neural Network"
      ],
      "metadata": {
        "id": "bR6LrYo3_3Ke"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "U_y-J0VM9Wo6"
      },
      "outputs": [],
      "source": [
        "## DEFINE CONVELUTIONAL NEURAL NETWORK MODEL\n",
        "\n",
        "class CONVModel(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(CONVModel, self).__init__()\n",
        "    self.conv1 = nn.Conv2d(3, 32, kernel_size=(3, 3), padding=(1, 1))\n",
        "    self.conv2 = nn.Conv2d(32, 64, kernel_size=(3, 3), padding=(1, 1))\n",
        "    self.pool1 = nn.MaxPool2d(2, 2)\n",
        "    self.pool2 = nn.MaxPool2d(2, 2)\n",
        "    self.flattener = nn.Flatten()\n",
        "    self.ln1 = nn.Linear(8*8*64, 64)\n",
        "    self.ln2 = nn.Linear(64, 10)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    x = self.pool1(F.relu(self.conv1(x)))\n",
        "    x = self.pool2(F.relu(self.conv2(x)))\n",
        "    x = self.flattener(x)\n",
        "    x = F.relu(self.ln1(x))\n",
        "    x = self.ln2(x)\n",
        "    return x\n",
        "\n",
        "CONVModel = CONVModel()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8-Layer Self-implemented Residual Neural Network\n",
        "### As per https://arxiv.org/pdf/1512.03385.pdf"
      ],
      "metadata": {
        "id": "euGJ9zYH_6Da"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "wBjaXBCxjRDi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a067f48-4cea-4c37-f4ed-41029e298ac6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
            "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
          ]
        }
      ],
      "source": [
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "  def __init__(self, channel_in, channel_out, downsample: bool = False):\n",
        "    super(ResidualBlock, self).__init__()\n",
        "    self.identity_map = nn.Sequential()\n",
        "\n",
        "    if downsample:\n",
        "      self.identity_map = nn.Sequential(\n",
        "          nn.Conv2d(channel_in, channel_out, kernel_size=(1,1), stride=2),\n",
        "          nn.BatchNorm2d(channel_out))\n",
        "\n",
        "    self.conv1 = nn.Conv2d(channel_in, channel_out, kernel_size=(3,3), stride=1, padding=1)\n",
        "    self.bNorm1 = nn.BatchNorm2d(channel_out)\n",
        "\n",
        "    if downsample:\n",
        "      self.conv1 = nn.Conv2d(channel_in, channel_out, kernel_size=(3,3), stride=2, padding=1)\n",
        "\n",
        "    self.conv2 = nn.Conv2d(channel_out, channel_out, kernel_size=(3,3), stride=1, padding=1)\n",
        "    self.bNorm2 = nn.BatchNorm2d(channel_out)\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    res = self.conv1(x)\n",
        "    res = F.relu(self.bNorm1(res))\n",
        "    res = self.conv2(res)\n",
        "    res = self.bNorm2(res)\n",
        "    res = F.relu(res)\n",
        "    return F.relu(res + self.identity_map(x))\n",
        "\n",
        "\n",
        "class ResidualNetSimple(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(ResidualNetSimple, self).__init__()\n",
        "\n",
        "    self.conv = nn.Conv2d(in_channels = 3, out_channels = 64, kernel_size = (3,3), stride=2)\n",
        "    self.pool = nn.MaxPool2d(kernel_size = (3,3), stride=1) # stride = 2\n",
        "\n",
        "    self.r1 = ResidualBlock(64, 64)\n",
        "    self.r2 = ResidualBlock(64, 64)\n",
        "    self.r3 = ResidualBlock(64, 128, True)\n",
        "    self.r4 = ResidualBlock(128, 128)\n",
        "    self.r5 = ResidualBlock(128, 256, True)\n",
        "    self.r6 = ResidualBlock(256, 256)\n",
        "    self.r7 = ResidualBlock(256, 512, True)\n",
        "    self.r8 = ResidualBlock(512, 512)\n",
        "  \n",
        "    self.avgpool = nn.AdaptiveAvgPool2d(1)\n",
        "    self.flatten = nn.Flatten()\n",
        "    self.linear = nn.LazyLinear(out_features=10)\n",
        "\n",
        "  def forward(self, x):\n",
        "\n",
        "    res = F.relu(self.conv(x))\n",
        "    res = F.relu(self.pool(res))\n",
        "\n",
        "    res = self.r1(res)\n",
        "    res = self.r2(res)\n",
        "    res = self.r3(res)\n",
        "    res = self.r4(res)\n",
        "    res = self.r5(res)\n",
        "    res = self.r6(res)\n",
        "    res = self.r7(res)\n",
        "    res = self.r8(res)\n",
        "\n",
        "    res = self.avgpool(res)\n",
        "\n",
        "    res = self.flatten(res)\n",
        "    res = self.linear(res)\n",
        "    return res\n",
        "\n",
        "ResModel = ResidualNetSimple()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 34-Layer Self-implemented Residual Neural Network"
      ],
      "metadata": {
        "id": "BlA823A_0gYS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ResidualBlock2(nn.Module):\n",
        "  def __init__(self, channel_in, channel_out, downsample=None , stride=1):\n",
        "    super(ResidualBlock2, self).__init__()\n",
        "    self.conv1 = nn.Conv2d(channel_in, channel_out, kernel_size=(3,3), stride=stride, padding=1)\n",
        "    self.bNorm1 = nn.BatchNorm2d(channel_out)\n",
        "    self.conv2 = nn.Conv2d(channel_out, channel_out, kernel_size=(3,3), stride=1, padding=1)\n",
        "    self.bNorm2 = nn.BatchNorm2d(channel_out)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.downsample = downsample\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    identity = x\n",
        "\n",
        "    x = self.conv1(x)\n",
        "    x = self.bNorm1(x)\n",
        "    x = self.relu(x)\n",
        "    x = self.conv2(x)\n",
        "    x = self.bNorm2(x)\n",
        "    x = self.relu(x)\n",
        "\n",
        "    if self.downsample is not None:\n",
        "      identity = self.downsample(identity)\n",
        "    # print(x.shape , identity.shape , self.conv1)\n",
        "    x = x + identity\n",
        "    x = self.relu(x)\n",
        "    return x\n",
        "\n",
        "class ResidualNetLarger(nn.Module): #[3,4,6,3]\n",
        "  def __init__(self , ResidualBlock2 , layers , img_channels, classes):\n",
        "    super(ResidualNetLarger, self).__init__()\n",
        "\n",
        "    self.channel_in = 64\n",
        "    self.conv1 = nn.Conv2d(img_channels, 64 , kernel_size=7, stride=2, padding=3)\n",
        "    self.bNorm1 = nn.BatchNorm2d(64)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "    self.layer1 = self.layer(ResidualBlock2, 64, layers[0], channel_out=64, stride=1)\n",
        "    self.layer2 = self.layer(ResidualBlock2, 64, layers[1], channel_out=128, stride=2)\n",
        "    self.layer3 = self.layer(ResidualBlock2, 128, layers[2], channel_out=256, stride=2)\n",
        "    self.layer4 = self.layer(ResidualBlock2, 256, layers[3], channel_out=512, stride=2)\n",
        "\n",
        "    self.avgpool = nn.AdaptiveAvgPool2d((1,1))\n",
        "    self.fc = nn.LazyLinear(classes)\n",
        "\n",
        "  def forward(self, x):\n",
        "      x = self.conv1(x)\n",
        "      x = self.bNorm1(x)\n",
        "      x = self.relu(x)\n",
        "      x = self.maxpool(x)\n",
        "      \n",
        "      x = self.layer1(x)\n",
        "      x = self.layer2(x)\n",
        "      x = self.layer3(x)\n",
        "      x = self.layer4(x)\n",
        "\n",
        "      x = self.avgpool(x)\n",
        "      x = nn.Flatten()(x)\n",
        "      x = self.fc(x)\n",
        "\n",
        "      return x\n",
        "\n",
        "\n",
        "  def layer(self, ResidualBlock2, channel_in, count_residual_blocks, channel_out, stride):\n",
        "    downsample = None\n",
        "    layers = []\n",
        "\n",
        "    if stride != 1 or channel_in != channel_out:\n",
        "      downsample = nn.Sequential(nn.Conv2d(channel_in, channel_out, kernel_size=1, stride=stride),\n",
        "                                 nn.BatchNorm2d(channel_out))\n",
        "      \n",
        "    layers.append(ResidualBlock2(channel_in, channel_out, downsample, stride))\n",
        "    channel_in = channel_out\n",
        "\n",
        "    for i in range(count_residual_blocks-1):\n",
        "      layers.append(ResidualBlock2(channel_in, channel_out))\n",
        "\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "def ResNet34(img_channel=3 , classes = 10):\n",
        "  return ResidualNetLarger(ResidualBlock2, [3, 4, 6, 3], img_channel, classes)\n",
        "\n",
        "Res34Model = ResNet34()"
      ],
      "metadata": {
        "id": "U_hQUIJ61NTl"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CIFAR-10 ResNet (self-implemented)"
      ],
      "metadata": {
        "id": "WYVfLsV_VBaM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CIFAR10Layers(nn.Module):\n",
        "  def __init__(self, channel_in, channel_out, downsample: bool = False):\n",
        "    super(CIFAR10Layers, self).__init__()\n",
        "    self.identity_map = nn.Sequential()\n",
        "\n",
        "    self.conv1 = nn.Conv2d(channel_in, channel_out, kernel_size=(3,3), stride=1, padding=1)\n",
        "    self.conv2 = nn.Conv2d(channel_out, channel_out, kernel_size=(3,3), stride=1, padding=1)\n",
        "\n",
        "    self.bnorm1 = nn.BatchNorm2d(channel_out)\n",
        "    self.bnorm2 = nn.BatchNorm2d(channel_out)\n",
        "\n",
        "    if downsample:\n",
        "      self.conv1 = nn.Conv2d(channel_in, channel_out, kernel_size=(3,3), stride=2, padding=1)\n",
        "      self.identity_map = nn.Sequential(\n",
        "          nn.Conv2d(channel_in, channel_out, kernel_size=(1,1), stride=2, padding=0)\n",
        "      )\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    res = F.relu(self.bnorm1(self.conv1(x)))\n",
        "    res = F.relu(self.bnorm2(self.conv2(res)))\n",
        "    return F.relu(res + self.identity_map(x))\n",
        "\n",
        "\n",
        "class ResidualNetCifar10(nn.Module):\n",
        "  \"\"\"ResNet for CIFAR10\n",
        "      As described in section 4.2 in https://arxiv.org/pdf/1512.03385.pdf\n",
        "\n",
        "  \"\"\"\n",
        "  def __init__(self):\n",
        "    super(ResidualNetCifar10, self).__init__()\n",
        "\n",
        "    self.conv = nn.Conv2d(in_channels = 3, out_channels = 16, kernel_size = (3,3), stride=1)\n",
        "  \n",
        "    self.l11 = CIFAR10Layers(16, 16, downsample=False)\n",
        "    self.l12 = CIFAR10Layers(16, 16, downsample=False)\n",
        "    self.l13 = CIFAR10Layers(16, 16, downsample=False)\n",
        "    self.l21 = CIFAR10Layers(16, 32, downsample=True)\n",
        "    self.l22 = CIFAR10Layers(32, 32, downsample=False)\n",
        "    self.l23 = CIFAR10Layers(32, 32, downsample=False)\n",
        "    self.l31 = CIFAR10Layers(32, 64, downsample=True)\n",
        "    self.l32 = CIFAR10Layers(64, 64, downsample=False)\n",
        "    self.l33 = CIFAR10Layers(64, 64, downsample=False)\n",
        "\n",
        "    self.avgpool = nn.AdaptiveAvgPool2d(1)\n",
        "    self.flatten = nn.Flatten()\n",
        "    self.linear = nn.LazyLinear(out_features=10)\n",
        "\n",
        "  def forward(self, x):\n",
        "\n",
        "    res = F.relu(self.conv(x))\n",
        "\n",
        "    res = self.l11(res)\n",
        "    res = self.l12(res)\n",
        "    res = self.l13(res)\n",
        "    res = self.l21(res)\n",
        "    res = self.l22(res)\n",
        "    res = self.l23(res)\n",
        "    res = self.l31(res)\n",
        "    res = self.l32(res)\n",
        "    res = self.l33(res)\n",
        "\n",
        "    res = self.avgpool(res)\n",
        "\n",
        "    res = self.flatten(res)\n",
        "    res = self.linear(res)\n",
        "    return res\n",
        "\n",
        "CifarResNet = ResidualNetCifar10()\n",
        "CifarResNet"
      ],
      "metadata": {
        "id": "mh7qKUP-VC7l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ResNet Github\n"
      ],
      "metadata": {
        "id": "LdtbMyZXC4pW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def conv_block(in_channels, out_channels, pool=False):\n",
        "    layers = [nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1), \n",
        "              nn.BatchNorm2d(out_channels), \n",
        "              nn.ReLU(inplace=True)]\n",
        "    if pool: layers.append(nn.MaxPool2d(2))\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "class ResNet9(nn.Module):\n",
        "    def __init__(self, in_channels, num_classes):\n",
        "        super(ResNet9, self).__init__()\n",
        "        \n",
        "        self.conv1 = conv_block(in_channels, 64)\n",
        "        self.conv2 = conv_block(64, 128, pool=True)\n",
        "        self.res1 = nn.Sequential(conv_block(128, 128), conv_block(128, 128))\n",
        "        \n",
        "        self.conv3 = conv_block(128, 256, pool=True)\n",
        "        self.conv4 = conv_block(256, 512, pool=True)\n",
        "        self.res2 = nn.Sequential(conv_block(512, 512), conv_block(512, 512))\n",
        "        \n",
        "        self.classifier = nn.Sequential(nn.MaxPool2d(4), \n",
        "                                        nn.Flatten(), \n",
        "                                        nn.Dropout(0.2),\n",
        "                                        nn.Linear(512, num_classes))\n",
        "        \n",
        "    def forward(self, xb):\n",
        "        out = self.conv1(xb)\n",
        "        out = self.conv2(out)\n",
        "        out = self.res1(out) + out\n",
        "        out = self.conv3(out)\n",
        "        out = self.conv4(out)\n",
        "        out = self.res2(out) + out\n",
        "        out = self.classifier(out)\n",
        "        return out\n",
        "\n",
        "MM = ResNet9(3, 10)\n",
        "MM"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SXTe2UlQC8mN",
        "outputId": "cd8d4619-ddec-49df-8fdc-f0acfa69f26d"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ResNet9(\n",
              "  (conv1): Sequential(\n",
              "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU(inplace=True)\n",
              "  )\n",
              "  (conv2): Sequential(\n",
              "    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU(inplace=True)\n",
              "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (res1): Sequential(\n",
              "    (0): Sequential(\n",
              "      (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU(inplace=True)\n",
              "    )\n",
              "    (1): Sequential(\n",
              "      (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (conv3): Sequential(\n",
              "    (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU(inplace=True)\n",
              "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (conv4): Sequential(\n",
              "    (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU(inplace=True)\n",
              "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (res2): Sequential(\n",
              "    (0): Sequential(\n",
              "      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU(inplace=True)\n",
              "    )\n",
              "    (1): Sequential(\n",
              "      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (classifier): Sequential(\n",
              "    (0): MaxPool2d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)\n",
              "    (1): Flatten(start_dim=1, end_dim=-1)\n",
              "    (2): Dropout(p=0.2, inplace=False)\n",
              "    (3): Linear(in_features=512, out_features=10, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Tests\n",
        "Simple assertions on input & output"
      ],
      "metadata": {
        "id": "rEysTet-AGc4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "mMi-QZPFVERC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "outputId": "383c8175-7c0e-4aec-f768-21d8cb76e1d1"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-635a6f983456>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0moutput4\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0moutput5\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTorchResModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0moutput5\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'TorchResModel' is not defined"
          ]
        }
      ],
      "source": [
        "## TEST BATCH SIZE 64\n",
        "test_data = torch.rand(64, 3, 32, 32)\n",
        "output = DNNModel(test_data)\n",
        "assert output.shape == (64, 10)\n",
        "\n",
        "output2 = CONVModel(test_data)\n",
        "assert output2.shape == (64, 10)\n",
        "\n",
        "output3 = ResModel(test_data)\n",
        "assert output3.shape == (64, 10)\n",
        "\n",
        "output4 = Res34Model(test_data)\n",
        "assert output4.shape == (64, 10)\n",
        "\n",
        "#output5 = TorchResModel(test_data)\n",
        "#assert output5.shape == (64, 1000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kczHF3VwW_xw"
      },
      "outputs": [],
      "source": [
        "# OPTIMIZER & LOSS FUNCTION\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "optimizer1 = torch.optim.Adam(DNNModel.parameters(), lr=1e-02)\n",
        "optimizer2 = torch.optim.Adam(CONVModel.parameters(), lr=1e-02)\n",
        "optimizer3 = torch.optim.Adam(ResModel.parameters(), lr=1e-02)\n",
        "optimizer4 = torch.optim.Adam(Res34Model.parameters(), lr=1e-02)\n",
        "\n",
        "# Overfit to following values and target:\n",
        "training_values1 = torch.rand(10, 3, 32, 32)\n",
        "training_values2 = torch.rand(10, 3, 32, 32)\n",
        "training_values3 = torch.rand(10, 3, 32, 32)\n",
        "training_values4 = torch.rand(10, 3, 32, 32)\n",
        "training_target = torch.Tensor(list(range(10))[::-1]).long()\n",
        "\n",
        "for i in range(50):\n",
        "  val1, val2, val3, val4 = DNNModel(training_values1), CONVModel(training_values2), ResModel(training_values3), Res34Model(training_values4)\n",
        "  loss1, loss2, loss3, loss4 = loss_function(val1, training_target), loss_function(val2, training_target), loss_function(val3, training_target), loss_function(val4, training_target)\n",
        "  optimizer1.zero_grad()\n",
        "  optimizer2.zero_grad()\n",
        "  optimizer3.zero_grad()\n",
        "  optimizer4.zero_grad()\n",
        "  loss1.backward()\n",
        "  loss2.backward()\n",
        "  loss3.backward()\n",
        "  loss4.backward()\n",
        "  optimizer1.step()\n",
        "  optimizer2.step()\n",
        "  optimizer3.step()\n",
        "  optimizer4.step()\n",
        "\n",
        "print(torch.argmax(DNNModel(training_values1), dim=1), training_target)\n",
        "print(torch.argmax(CONVModel(training_values2), dim=1), training_target)\n",
        "print(torch.argmax(ResModel(training_values3), dim=1), training_target)\n",
        "print(torch.argmax(Res34Model(training_values4), dim=1), training_target)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Custom Definitions for data_loader and loss\n",
        " (No longer used in favor of built-in, more stable, implementations)"
      ],
      "metadata": {
        "id": "6AW-AN1NBECS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eMRlyqIr9SDS"
      },
      "outputs": [],
      "source": [
        "## LOSS FUNCTION\n",
        "# CROSS ENTROPY LOSS\n",
        "\n",
        "def cross_entropy_loss(logits: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n",
        "  def softmax_average(values: torch.Tensor, target: int):\n",
        "    return torch.exp(value[target]) / torch.sum(torch.exp(values))\n",
        "\n",
        "  if logits.shape[0] != len(target):\n",
        "    print(\"LOGITS SHAPE (BATCH, C) DOES NOT EQUAL TARGET SHAPE (C,)\")\n",
        "  else:\n",
        "    loss = 0\n",
        "    for index, value in enumerate(logits):\n",
        "      loss += -torch.log(softmax_average(value, target[index]))\n",
        "    return loss\n",
        "\n",
        "def get_new_batch(dataset, index: int, take_out_last: bool = True):\n",
        "\n",
        "  tmp_values  = torch.zeros((batch_size, 3, 32, 32))\n",
        "  tmp_targets = torch.zeros((batch_size))\n",
        "  try:\n",
        "    for f, i in enumerate(range(index, index + batch_size)):\n",
        "      tmp_values[f] = dataset[i][0] / 255.0\n",
        "      tmp_targets[f] = dataset[i][1]\n",
        "  except IndexError as E:\n",
        "    if take_out_last:\n",
        "      return None, None\n",
        "  return tmp_values.to(device), tmp_targets.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main Training Loop"
      ],
      "metadata": {
        "id": "K72ChULfBPmV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "369McNyvatjE",
        "outputId": "6a2f75fa-8935-4e58-9bfd-649c0f460067"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----- EPOCH 1 -----\n",
            "\t\tBATCH 100:\tBatch Loss: 352.64\tMean loss: 2.76\t\n",
            "\t\tBATCH 200:\tBatch Loss: 211.62\tMean loss: 1.65\t\n",
            "\t\tBATCH 300:\tBatch Loss: 180.92\tMean loss: 1.41\t\n",
            "----- END OF EPOCH 1 -----\n",
            " *** RUNNING VALIDATION TEST ***\n",
            "\tTest Accuracy: 0.4632\n",
            "----- EPOCH 2 -----\n",
            "\t\tBATCH 100:\tBatch Loss: 141.90\tMean loss: 1.11\t\n",
            "\t\tBATCH 200:\tBatch Loss: 126.33\tMean loss: 0.99\t\n",
            "\t\tBATCH 300:\tBatch Loss: 113.00\tMean loss: 0.88\t\n",
            "----- END OF EPOCH 2 -----\n",
            " *** RUNNING VALIDATION TEST ***\n",
            "\tTest Accuracy: 0.5353\n",
            "----- EPOCH 3 -----\n",
            "\t\tBATCH 100:\tBatch Loss: 94.46\tMean loss: 0.74\t\n",
            "\t\tBATCH 200:\tBatch Loss: 91.61\tMean loss: 0.72\t\n",
            "\t\tBATCH 300:\tBatch Loss: 84.29\tMean loss: 0.66\t\n",
            "----- END OF EPOCH 3 -----\n",
            " *** RUNNING VALIDATION TEST ***\n",
            "\tTest Accuracy: 0.6575\n",
            "----- EPOCH 4 -----\n",
            "\t\tBATCH 100:\tBatch Loss: 81.01\tMean loss: 0.63\t\n",
            "\t\tBATCH 200:\tBatch Loss: 79.75\tMean loss: 0.62\t\n",
            "\t\tBATCH 300:\tBatch Loss: 78.35\tMean loss: 0.61\t\n",
            "----- END OF EPOCH 4 -----\n",
            " *** RUNNING VALIDATION TEST ***\n",
            "\tTest Accuracy: 0.7278\n",
            "----- EPOCH 5 -----\n",
            "\t\tBATCH 100:\tBatch Loss: 73.63\tMean loss: 0.58\t\n",
            "\t\tBATCH 200:\tBatch Loss: 75.60\tMean loss: 0.59\t\n",
            "\t\tBATCH 300:\tBatch Loss: 73.68\tMean loss: 0.58\t\n",
            "----- END OF EPOCH 5 -----\n",
            " *** RUNNING VALIDATION TEST ***\n",
            "\tTest Accuracy: 0.7232\n",
            "----- EPOCH 6 -----\n",
            "\t\tBATCH 100:\tBatch Loss: 72.35\tMean loss: 0.57\t\n",
            "\t\tBATCH 200:\tBatch Loss: 72.75\tMean loss: 0.57\t\n",
            "\t\tBATCH 300:\tBatch Loss: 71.25\tMean loss: 0.56\t\n",
            "----- END OF EPOCH 6 -----\n",
            " *** RUNNING VALIDATION TEST ***\n",
            "\tTest Accuracy: 0.7506\n",
            "----- EPOCH 7 -----\n",
            "\t\tBATCH 100:\tBatch Loss: 67.76\tMean loss: 0.53\t\n",
            "\t\tBATCH 200:\tBatch Loss: 68.45\tMean loss: 0.53\t\n",
            "\t\tBATCH 300:\tBatch Loss: 69.35\tMean loss: 0.54\t\n",
            "----- END OF EPOCH 7 -----\n",
            " *** RUNNING VALIDATION TEST ***\n",
            "\tTest Accuracy: 0.7372\n",
            "----- EPOCH 8 -----\n",
            "\t\tBATCH 100:\tBatch Loss: 67.70\tMean loss: 0.53\t\n",
            "\t\tBATCH 200:\tBatch Loss: 66.86\tMean loss: 0.52\t\n",
            "\t\tBATCH 300:\tBatch Loss: 66.82\tMean loss: 0.52\t\n",
            "----- END OF EPOCH 8 -----\n",
            " *** RUNNING VALIDATION TEST ***\n",
            "Epoch 00008: reducing learning rate of group 0 to 1.0000e-03.\n",
            "\tTest Accuracy: 0.7502\n",
            "----- EPOCH 9 -----\n",
            "\t\tBATCH 100:\tBatch Loss: 56.13\tMean loss: 0.44\t\n",
            "\t\tBATCH 200:\tBatch Loss: 50.50\tMean loss: 0.39\t\n",
            "\t\tBATCH 300:\tBatch Loss: 49.01\tMean loss: 0.38\t\n",
            "----- END OF EPOCH 9 -----\n",
            " *** RUNNING VALIDATION TEST ***\n",
            "\tTest Accuracy: 0.8477\n",
            "----- EPOCH 10 -----\n",
            "\t\tBATCH 100:\tBatch Loss: 44.76\tMean loss: 0.35\t\n",
            "\t\tBATCH 200:\tBatch Loss: 44.77\tMean loss: 0.35\t\n",
            "\t\tBATCH 300:\tBatch Loss: 43.26\tMean loss: 0.34\t\n",
            "----- END OF EPOCH 10 -----\n",
            " *** RUNNING VALIDATION TEST ***\n",
            "\tTest Accuracy: 0.8573\n",
            "----- EPOCH 11 -----\n",
            "\t\tBATCH 100:\tBatch Loss: 41.30\tMean loss: 0.32\t\n",
            "\t\tBATCH 200:\tBatch Loss: 40.02\tMean loss: 0.31\t\n",
            "\t\tBATCH 300:\tBatch Loss: 41.39\tMean loss: 0.32\t\n",
            "----- END OF EPOCH 11 -----\n",
            " *** RUNNING VALIDATION TEST ***\n",
            "\tTest Accuracy: 0.8631\n",
            "----- EPOCH 12 -----\n",
            "\t\tBATCH 100:\tBatch Loss: 38.41\tMean loss: 0.30\t\n",
            "\t\tBATCH 200:\tBatch Loss: 39.29\tMean loss: 0.31\t\n",
            "\t\tBATCH 300:\tBatch Loss: 38.42\tMean loss: 0.30\t\n",
            "----- END OF EPOCH 12 -----\n",
            " *** RUNNING VALIDATION TEST ***\n",
            "\tTest Accuracy: 0.8681\n",
            "----- EPOCH 13 -----\n",
            "\t\tBATCH 100:\tBatch Loss: 34.23\tMean loss: 0.27\t\n",
            "\t\tBATCH 200:\tBatch Loss: 37.60\tMean loss: 0.29\t\n",
            "\t\tBATCH 300:\tBatch Loss: 36.76\tMean loss: 0.29\t\n",
            "----- END OF EPOCH 13 -----\n",
            " *** RUNNING VALIDATION TEST ***\n",
            "\tTest Accuracy: 0.873\n",
            "----- EPOCH 14 -----\n",
            "\t\tBATCH 100:\tBatch Loss: 34.31\tMean loss: 0.27\t\n",
            "\t\tBATCH 200:\tBatch Loss: 34.16\tMean loss: 0.27\t\n",
            "\t\tBATCH 300:\tBatch Loss: 34.28\tMean loss: 0.27\t\n",
            "----- END OF EPOCH 14 -----\n",
            " *** RUNNING VALIDATION TEST ***\n",
            "\tTest Accuracy: 0.8705\n",
            "----- EPOCH 15 -----\n",
            "\t\tBATCH 100:\tBatch Loss: 30.75\tMean loss: 0.24\t\n",
            "\t\tBATCH 200:\tBatch Loss: 33.29\tMean loss: 0.26\t\n",
            "\t\tBATCH 300:\tBatch Loss: 32.99\tMean loss: 0.26\t\n",
            "----- END OF EPOCH 15 -----\n",
            " *** RUNNING VALIDATION TEST ***\n",
            "\tTest Accuracy: 0.8718\n",
            "----- EPOCH 16 -----\n",
            "\t\tBATCH 100:\tBatch Loss: 31.46\tMean loss: 0.25\t\n",
            "\t\tBATCH 200:\tBatch Loss: 31.90\tMean loss: 0.25\t\n",
            "\t\tBATCH 300:\tBatch Loss: 31.32\tMean loss: 0.24\t\n",
            "----- END OF EPOCH 16 -----\n",
            " *** RUNNING VALIDATION TEST ***\n",
            "Epoch 00016: reducing learning rate of group 0 to 1.0000e-04.\n",
            "\tTest Accuracy: 0.8762\n",
            "----- EPOCH 17 -----\n",
            "\t\tBATCH 100:\tBatch Loss: 27.97\tMean loss: 0.22\t\n",
            "\t\tBATCH 200:\tBatch Loss: 27.09\tMean loss: 0.21\t\n",
            "\t\tBATCH 300:\tBatch Loss: 27.10\tMean loss: 0.21\t\n",
            "----- END OF EPOCH 17 -----\n",
            " *** RUNNING VALIDATION TEST ***\n",
            "\tTest Accuracy: 0.8864\n",
            "----- EPOCH 18 -----\n",
            "\t\tBATCH 100:\tBatch Loss: 25.96\tMean loss: 0.20\t\n",
            "\t\tBATCH 200:\tBatch Loss: 26.71\tMean loss: 0.21\t\n",
            "\t\tBATCH 300:\tBatch Loss: 25.94\tMean loss: 0.20\t\n",
            "----- END OF EPOCH 18 -----\n",
            " *** RUNNING VALIDATION TEST ***\n",
            "\tTest Accuracy: 0.887\n",
            "----- EPOCH 19 -----\n",
            "\t\tBATCH 100:\tBatch Loss: 24.83\tMean loss: 0.19\t\n",
            "\t\tBATCH 200:\tBatch Loss: 25.23\tMean loss: 0.20\t\n",
            "\t\tBATCH 300:\tBatch Loss: 25.23\tMean loss: 0.20\t\n",
            "----- END OF EPOCH 19 -----\n",
            " *** RUNNING VALIDATION TEST ***\n",
            "\tTest Accuracy: 0.8896\n",
            "----- EPOCH 20 -----\n",
            "\t\tBATCH 100:\tBatch Loss: 24.48\tMean loss: 0.19\t\n",
            "\t\tBATCH 200:\tBatch Loss: 25.87\tMean loss: 0.20\t\n",
            "\t\tBATCH 300:\tBatch Loss: 23.90\tMean loss: 0.19\t\n",
            "----- END OF EPOCH 20 -----\n",
            " *** RUNNING VALIDATION TEST ***\n",
            "\tTest Accuracy: 0.8899\n",
            "----- EPOCH 21 -----\n",
            "\t\tBATCH 100:\tBatch Loss: 24.58\tMean loss: 0.19\t\n",
            "\t\tBATCH 200:\tBatch Loss: 24.46\tMean loss: 0.19\t\n",
            "\t\tBATCH 300:\tBatch Loss: 24.55\tMean loss: 0.19\t\n",
            "----- END OF EPOCH 21 -----\n",
            " *** RUNNING VALIDATION TEST ***\n",
            "\tTest Accuracy: 0.8895\n",
            "----- EPOCH 22 -----\n",
            "\t\tBATCH 100:\tBatch Loss: 24.68\tMean loss: 0.19\t\n",
            "\t\tBATCH 200:\tBatch Loss: 23.66\tMean loss: 0.18\t\n",
            "\t\tBATCH 300:\tBatch Loss: 23.47\tMean loss: 0.18\t\n",
            "----- END OF EPOCH 22 -----\n",
            " *** RUNNING VALIDATION TEST ***\n",
            "\tTest Accuracy: 0.8897\n",
            "----- EPOCH 23 -----\n",
            "\t\tBATCH 100:\tBatch Loss: 23.04\tMean loss: 0.18\t\n",
            "\t\tBATCH 200:\tBatch Loss: 24.15\tMean loss: 0.19\t\n",
            "\t\tBATCH 300:\tBatch Loss: 23.37\tMean loss: 0.18\t\n",
            "----- END OF EPOCH 23 -----\n",
            " *** RUNNING VALIDATION TEST ***\n",
            "\tTest Accuracy: 0.8922\n",
            "----- EPOCH 24 -----\n",
            "\t\tBATCH 100:\tBatch Loss: 23.81\tMean loss: 0.19\t\n",
            "\t\tBATCH 200:\tBatch Loss: 22.37\tMean loss: 0.17\t\n",
            "\t\tBATCH 300:\tBatch Loss: 22.81\tMean loss: 0.18\t\n",
            "----- END OF EPOCH 24 -----\n",
            " *** RUNNING VALIDATION TEST ***\n",
            "\tTest Accuracy: 0.8917\n",
            "----- EPOCH 25 -----\n",
            "\t\tBATCH 100:\tBatch Loss: 22.59\tMean loss: 0.18\t\n",
            "\t\tBATCH 200:\tBatch Loss: 22.46\tMean loss: 0.18\t\n",
            "\t\tBATCH 300:\tBatch Loss: 23.30\tMean loss: 0.18\t\n",
            "----- END OF EPOCH 25 -----\n",
            " *** RUNNING VALIDATION TEST ***\n",
            "\tTest Accuracy: 0.8944\n",
            "----- EPOCH 26 -----\n",
            "\t\tBATCH 100:\tBatch Loss: 23.18\tMean loss: 0.18\t\n",
            "\t\tBATCH 200:\tBatch Loss: 21.21\tMean loss: 0.17\t\n",
            "\t\tBATCH 300:\tBatch Loss: 22.39\tMean loss: 0.17\t\n",
            "----- END OF EPOCH 26 -----\n",
            " *** RUNNING VALIDATION TEST ***\n",
            "\tTest Accuracy: 0.8935\n",
            "----- EPOCH 27 -----\n",
            "\t\tBATCH 100:\tBatch Loss: 22.81\tMean loss: 0.18\t\n",
            "\t\tBATCH 200:\tBatch Loss: 22.73\tMean loss: 0.18\t\n",
            "\t\tBATCH 300:\tBatch Loss: 21.89\tMean loss: 0.17\t\n",
            "----- END OF EPOCH 27 -----\n",
            " *** RUNNING VALIDATION TEST ***\n",
            "\tTest Accuracy: 0.8937\n",
            "----- EPOCH 28 -----\n",
            "\t\tBATCH 100:\tBatch Loss: 21.42\tMean loss: 0.17\t\n",
            "\t\tBATCH 200:\tBatch Loss: 21.57\tMean loss: 0.17\t\n",
            "\t\tBATCH 300:\tBatch Loss: 21.32\tMean loss: 0.17\t\n",
            "----- END OF EPOCH 28 -----\n",
            " *** RUNNING VALIDATION TEST ***\n",
            "\tTest Accuracy: 0.8946\n",
            "----- EPOCH 29 -----\n",
            "\t\tBATCH 100:\tBatch Loss: 20.85\tMean loss: 0.16\t\n",
            "\t\tBATCH 200:\tBatch Loss: 20.95\tMean loss: 0.16\t\n",
            "\t\tBATCH 300:\tBatch Loss: 21.48\tMean loss: 0.17\t\n",
            "----- END OF EPOCH 29 -----\n",
            " *** RUNNING VALIDATION TEST ***\n",
            "\tTest Accuracy: 0.8932\n",
            "----- EPOCH 30 -----\n",
            "\t\tBATCH 100:\tBatch Loss: 20.48\tMean loss: 0.16\t\n",
            "\t\tBATCH 200:\tBatch Loss: 21.19\tMean loss: 0.17\t\n",
            "\t\tBATCH 300:\tBatch Loss: 21.17\tMean loss: 0.17\t\n",
            "----- END OF EPOCH 30 -----\n",
            " *** RUNNING VALIDATION TEST ***\n",
            "Epoch 00030: reducing learning rate of group 0 to 1.0000e-05.\n",
            "\tTest Accuracy: 0.8945\n",
            "----- EPOCH 31 -----\n",
            "\t\tBATCH 100:\tBatch Loss: 20.72\tMean loss: 0.16\t\n",
            "\t\tBATCH 200:\tBatch Loss: 20.55\tMean loss: 0.16\t\n",
            "\t\tBATCH 300:\tBatch Loss: 18.98\tMean loss: 0.15\t\n",
            "----- END OF EPOCH 31 -----\n",
            " *** RUNNING VALIDATION TEST ***\n",
            "\tTest Accuracy: 0.8959\n",
            "----- EPOCH 32 -----\n",
            "\t\tBATCH 100:\tBatch Loss: 19.88\tMean loss: 0.16\t\n",
            "\t\tBATCH 200:\tBatch Loss: 20.30\tMean loss: 0.16\t\n",
            "\t\tBATCH 300:\tBatch Loss: 19.24\tMean loss: 0.15\t\n",
            "----- END OF EPOCH 32 -----\n",
            " *** RUNNING VALIDATION TEST ***\n",
            "\tTest Accuracy: 0.8949\n",
            "----- EPOCH 33 -----\n"
          ]
        }
      ],
      "source": [
        "# Define neccessary variables\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "epochs = 100\n",
        "batch_size = 128\n",
        "\n",
        "continueTrain = False\n",
        "saveModel = True\n",
        "loadModel = False\n",
        "\n",
        "if not continueTrain:\n",
        "  model = ResNet9(3, 10)\n",
        "\n",
        "model.to(device) \n",
        "\n",
        "if loadModel:\n",
        "  model.load_state_dict(torch.load(\"/content/drive/MyDrive/DL/model_params.pt\"))\n",
        "else:\n",
        "  # DATA LOADERS\n",
        "  test_loader = get_test_loader(batch_size)\n",
        "  train_loader = get_train_loader(batch_size)\n",
        "\n",
        "  # OPTIMIZER & LOSS FUNCTION\n",
        "  loss_function = nn.CrossEntropyLoss()\n",
        "  optimizer = torch.optim.Adam(model.parameters(), weight_decay=1e-04, lr=1e-02)\n",
        "  scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=1, verbose=True)\n",
        "\n",
        "  val_losses = []\n",
        "\n",
        "  #####################\n",
        "  # INITIAL TEST LOSS #\n",
        "  #####################\n",
        "\n",
        "  val_loss = 0\n",
        "  for val, targ in iter(test_loader):\n",
        "    loss = loss_function(model(val.to(device)), targ.to(device))\n",
        "    val_loss += loss.sum().item()\n",
        "  val_losses.append(val_loss)\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    print(f\"----- EPOCH {epoch + 1} -----\")\n",
        "\n",
        "    ##################\n",
        "    # MAIN TEST LOOP #\n",
        "    ##################\n",
        "\n",
        "    running_loss = 0\n",
        "    model.train()\n",
        "\n",
        "    for batch, (vals, real_targs) in enumerate(iter(get_train_loader(batch_size))):\n",
        "      vals, real_targs = vals.to(device), real_targs.to(device)\n",
        "      model_targs = model(vals)\n",
        "\n",
        "      # zero grad the optimizer\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      # calculate loss\n",
        "      loss = loss_function(model_targs, real_targs)\n",
        "      running_loss += loss.item()\n",
        "      # Backpropogate loss\n",
        "      loss.backward()\n",
        "\n",
        "      # Step optimizer\n",
        "      optimizer.step()\n",
        "\n",
        "      if (batch + 1) % 100 == 0:\n",
        "        print(f\"\\t\\tBATCH {(batch+1)}:\\tBatch Loss: {running_loss:.2f}\\tMean loss: {(running_loss/batch_size):.2f}\\t\")\n",
        "        running_loss = 0\n",
        "      \n",
        "    ###########################\n",
        "    # END-OF-EPOCH VALIDATION #\n",
        "    ###########################\n",
        "\n",
        "    print(f\"----- END OF EPOCH {epoch + 1} -----\")\n",
        "    print(f\" *** RUNNING VALIDATION TEST ***\")\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    for val, targ in iter(test_loader):\n",
        "      val_loss += loss_function(model(val.to(device)), targ.to(device)).item()\n",
        "    scheduler.step(val_loss)\n",
        "    val_losses.append(val_loss)\n",
        "    wrong, correct = 0, 0\n",
        "\n",
        "    for val, targ in iter(get_test_loader(batch_size=32)):\n",
        "      val, targ = val.to(device), targ.to(device)\n",
        "\n",
        "      val = model(val)\n",
        "      for v, t in zip(val, targ):\n",
        "        if torch.argmax(v).item() == t.item():\n",
        "          correct += 1\n",
        "        else:\n",
        "          wrong += 1\n",
        "    print(f\"\\tTest Accuracy: {correct/(correct+wrong)}\")\n",
        "  print(f\"DONE!\")\n",
        "\n",
        "  if saveModel:\n",
        "    torch.save(model.state_dict(), \"/content/drive/MyDrive/DL/model_params.pt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Plots & Statistics"
      ],
      "metadata": {
        "id": "8ljIYg3EEIHA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Conf Matrix"
      ],
      "metadata": {
        "id": "fKKaZQ6lQFh1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VWyFAwvpi6ri"
      },
      "outputs": [],
      "source": [
        "# Confusion Matrix:\n",
        "predictions = []\n",
        "real = []\n",
        "\n",
        "for v, t in iter(get_test_loader(32)):\n",
        "  predictions.extend(model(v.to(device)).argmax(1).cpu())\n",
        "  real.extend(t.cpu())\n",
        "\n",
        "#The following 7 lines are all that is required to plot the confusion matrix.\n",
        "predictions_for_cm = predictions\n",
        "\n",
        "class_names = [\"airplane\",\"automobile\",\"bird\",\"cat\",\"deer\",\"dog\",\"frog\",\"horse\",\"ship\",\"truck\"]\n",
        "\n",
        "cm = confusion_matrix(real,predictions_for_cm)\n",
        "plt.figure(figsize=(8,8))\n",
        "sns.heatmap(cm, annot=True,  fmt=\".0f\", xticklabels=class_names, yticklabels = class_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Accuracy"
      ],
      "metadata": {
        "id": "3GjPRxSiQJUD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Ke-kS02kmI_"
      },
      "outputs": [],
      "source": [
        "wrong, correct = 0, 0\n",
        "\n",
        "for val, targ in iter(get_test_loader(batch_size=32)):\n",
        "  val, targ = val.to(device), targ.to(device)\n",
        "\n",
        "  val = model(val)\n",
        "  for v, t in zip(val, targ):\n",
        "    if torch.argmax(v).item() == t.item():\n",
        "      correct += 1\n",
        "    else:\n",
        "      wrong += 1\n",
        "\n",
        "print(f\"Total: {wrong+correct} ; Wrong: {wrong}, Correct: {correct}, average: {correct/(wrong+correct) * 100.0}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plot Images"
      ],
      "metadata": {
        "id": "8nB0uRk8QM6f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "tao-KL6dMLb9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Confidence Interval\n"
      ],
      "metadata": {
        "id": "cazqUIgGQOk4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "###################################\n",
        "# Confidence interval for models: #\n",
        "###################################\n",
        "\n",
        "# IF confidence interval of proportion is 10% > p > 90%\n",
        "\n",
        "# name: Agresti-Coull interval\n",
        "\n",
        "#Sample size\n",
        "n = 10000\n",
        "\n",
        "def Conf_interval(p: float):\n",
        "  if p < 0.1 or p > 0.9:\n",
        "    p_hat = (p*n+2)/(n+4)\n",
        "    n_hat = n+4\n",
        "\n",
        "    Agresti_Coull_upper = p_hat + 1.96 * math.sqrt((p_hat*(1-p_hat))/n_hat)\n",
        "    Agresti_Coull_lower = p_hat - 1.96 * math.sqrt((p_hat*(1-p_hat))/n_hat)\n",
        "\n",
        "    return Agresti_Coull_lower , Agresti_Coull_upper\n",
        "\n",
        "  else:\n",
        "    upper = p + 1.96 * math.sqrt(p*((1-p)/n))\n",
        "    lower = p - 1.96 * math.sqrt(p*((1-p)/n))\n",
        "    return upper , lower\n"
      ],
      "metadata": {
        "id": "O-zwJtPcQQa9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing Area"
      ],
      "metadata": {
        "id": "dl7JSKIn5VA1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TData = CIFAR10(\"/cifardata\", transform=Compose([ToTensor(), RandomCrop(32, 4), RandomHorizontalFlip()]), train=True)\n",
        "tLoader = DataLoader(TData, batch_size = 1, shuffle=False)\n",
        "\n",
        "imgs = []\n",
        "for i in range(9):\n",
        "  img, _ = next(iter(tLoader))\n",
        "  imgs.extend(img)\n",
        "\n",
        "plt.figure(figsize=(10,10))\n",
        "\n",
        "for i, img in enumerate(imgs):\n",
        "  if i==9:\n",
        "    break\n",
        "  ax = plt.subplot(3, 3, i + 1)\n",
        "  imshow(img.permute(1, 2, 0))\n",
        "  plt.axis(\"off\")\n",
        "\n"
      ],
      "metadata": {
        "id": "Hx-ETi4_5Wvw"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "bb120bb2b6c747e4897081f1daaddd5d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_51abb984d3134d0fba9f3507234986bf",
              "IPY_MODEL_c00ebf1423a64e1d801a2d7c88de01d0",
              "IPY_MODEL_fc504cb813b54098aa01f73bad7e517d"
            ],
            "layout": "IPY_MODEL_bea9ff9ba1e74254b736f4ce49146530"
          }
        },
        "51abb984d3134d0fba9f3507234986bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_776b3662a007401d9f2bd5941181dae4",
            "placeholder": "",
            "style": "IPY_MODEL_e9da124cdf344622bdeab1a319139eed",
            "value": "100%"
          }
        },
        "c00ebf1423a64e1d801a2d7c88de01d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_416ec78e87f14909b5d4305f20e16dbe",
            "max": 170498071,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_311b7ab6fa2845678bc3dde13394e5c2",
            "value": 170498071
          }
        },
        "fc504cb813b54098aa01f73bad7e517d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4f27c12eb54e4393ac7d66d8119f22d0",
            "placeholder": "",
            "style": "IPY_MODEL_568ef96c155643fda76a98e883b6f427",
            "value": " 170498071/170498071 [00:05&lt;00:00, 33845136.63it/s]"
          }
        },
        "bea9ff9ba1e74254b736f4ce49146530": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "776b3662a007401d9f2bd5941181dae4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e9da124cdf344622bdeab1a319139eed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "416ec78e87f14909b5d4305f20e16dbe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "311b7ab6fa2845678bc3dde13394e5c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4f27c12eb54e4393ac7d66d8119f22d0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "568ef96c155643fda76a98e883b6f427": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}